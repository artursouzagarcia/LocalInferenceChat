# Local Inference Chat ğŸ’¬

Um chat local com IA que utiliza modelos de linguagem diretamente no navegador, sem necessidade de servidor ou conexÃ£o com internet para a inferÃªncia.

## ğŸš€ CaracterÃ­sticas

- **InferÃªncia Local**: Executa modelos de IA diretamente no navegador usando WebAssembly
- **Suporte a MÃºltiplos Modelos**: CompatÃ­vel com modelos Gemma (2B, 3B, 3-1B)
- **Chat em Tempo Real**: Interface de chat com streaming de respostas
- **Upload de Imagens**: Suporte para envio e visualizaÃ§Ã£o de imagens
- **Interface Moderna**: UI responsiva e intuitiva
- **Sem Servidor**: Funciona completamente offline apÃ³s o carregamento inicial

## ğŸ› ï¸ Tecnologias Utilizadas

- **React 19**: Framework principal para a interface
- **Vite**: Build tool e servidor de desenvolvimento
- **MediaPipe Tasks GenAI**: Biblioteca do Google para inferÃªncia de IA no navegador
- **CSS3**: EstilizaÃ§Ã£o moderna da interface

## ğŸ“‹ PrÃ©-requisitos

- Node.js (versÃ£o 16 ou superior)
- npm ou yarn
- Navegador moderno com suporte a WebAssembly

## ğŸ”§ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone <url-do-repositorio>
cd LocalInferenceChat
```

2. Instale as dependÃªncias:
```bash
npm install
```

3. Inicie o servidor de desenvolvimento:
```bash
npm run dev
```

4. Abra o navegador em `http://localhost:5173`

## ğŸ“ Estrutura do Projeto

```
LocalInferenceChat/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ vite.svg
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ gemma2-2b-it-gpu-int8.bin      # Modelo Gemma 2B
â”‚   â”‚   â”œâ”€â”€ gemma-3n-E2B-it-int4.task      # Modelo Gemma 3B
â”‚   â”‚   â”œâ”€â”€ gemma3-1b-it-int4-web.task     # Modelo Gemma 3-1B
â”‚   â”‚   â””â”€â”€ react.svg
â”‚   â”œâ”€â”€ App.jsx                            # Componente principal
â”‚   â”œâ”€â”€ App.css                            # Estilos da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ index.css                          # Estilos globais
â”‚   â””â”€â”€ main.jsx                           # Ponto de entrada
â”œâ”€â”€ package.json
â”œâ”€â”€ vite.config.js
â””â”€â”€ README.md
```

## ğŸ§  Modelos Suportados

O projeto inclui trÃªs modelos Gemma prÃ©-configurados:

1. **Gemma 2B GPU INT8** (`gemma2-2b-it-gpu-int8.bin`)
   - Modelo de 2 bilhÃµes de parÃ¢metros
   - Otimizado para GPU com quantizaÃ§Ã£o INT8

2. **Gemma 3B E2B INT4** (`gemma-3n-E2B-it-int4.task`)
   - Modelo de 3 bilhÃµes de parÃ¢metros
   - QuantizaÃ§Ã£o INT4 para melhor performance

3. **Gemma 3-1B Web INT4** (`gemma3-1b-it-4-web.task`) - **PadrÃ£o**
   - Modelo de 1 bilhÃ£o de parÃ¢metros
   - Otimizado para execuÃ§Ã£o web

## ğŸ¯ Funcionalidades

### Chat de Texto
- Interface de chat intuitiva
- Respostas em streaming (tempo real)
- HistÃ³rico de conversas
- Indicador visual de digitaÃ§Ã£o da IA

### Upload de Imagens
- Suporte para upload de imagens
- VisualizaÃ§Ã£o inline das imagens enviadas
- Processamento local das imagens

### Controles
- BotÃ£o para limpar o chat
- Envio por Enter ou botÃ£o
- Interface responsiva

## ğŸ”„ Como Funciona

1. **InicializaÃ§Ã£o**: O modelo de IA Ã© carregado usando MediaPipe Tasks GenAI
2. **InferÃªncia Local**: As respostas sÃ£o geradas localmente no navegador
3. **Streaming**: As respostas sÃ£o exibidas em tempo real conforme sÃ£o geradas
4. **Processamento**: Tudo acontece no cliente, sem envio de dados para servidores

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### ParÃ¢metros do Modelo

No arquivo `App.jsx`, vocÃª pode ajustar os parÃ¢metros do modelo:

```javascript
LlmInference.createFromOptions(genaiFileset, {
    baseOptions: { modelAssetPath: modelFileName },
    maxTokens: 512,        // MÃ¡ximo de tokens (entrada + saÃ­da)
    randomSeed: 1,         // Seed para geraÃ§Ã£o determinÃ­stica
    topK: 1,              // NÃºmero de tokens considerados a cada passo
    temperature: 1.0,      // Randomicidade na geraÃ§Ã£o
})
```

### Trocar Modelo

Para usar um modelo diferente, altere a linha no `App.jsx`:

```javascript
const [modelFileName, setModelFileName] = useState(modelFilesNames[0]); // 0, 1 ou 2
```

## ğŸ“š Scripts DisponÃ­veis

- `npm run dev`: Inicia o servidor de desenvolvimento
- `npm run build`: Cria build de produÃ§Ã£o
- `npm run preview`: Visualiza build de produÃ§Ã£o
- `npm run lint`: Executa linting do cÃ³digo

## ğŸš€ Deploy

Para fazer deploy da aplicaÃ§Ã£o:

1. Crie o build de produÃ§Ã£o:
```bash
npm run build
```

2. Os arquivos estarÃ£o na pasta `dist/`

3. Hospede os arquivos em qualquer servidor web estÃ¡tico

**Nota**: Certifique-se de que o servidor suporte arquivos `.bin` e `.task` e tenha os headers CORS adequados para WebAssembly.

## ğŸ”— ReferÃªncias

Este projeto foi desenvolvido baseado na documentaÃ§Ã£o oficial do Google MediaPipe:
- [LLM Inference Web JS - MediaPipe](https://ai.google.dev/edge/mediapipe/solutions/genai/llm_inference/web_js?hl=pt-br#run-task)

## ğŸ¤ Contribuindo

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## âš ï¸ LimitaÃ§Ãµes

- **Performance**: A velocidade depende do hardware do usuÃ¡rio
- **MemÃ³ria**: Modelos maiores requerem mais RAM
- **Compatibilidade**: Necessita navegador com suporte a WebAssembly
- **Tamanho**: Os modelos tÃªm tamanhos considerÃ¡veis (centenas de MB)

## ğŸ”§ Troubleshooting

### Erro de InicializaÃ§Ã£o
- Verifique se o navegador suporta WebAssembly
- Certifique-se de que os arquivos de modelo estÃ£o na pasta correta

### Performance Lenta
- Experimente modelos menores (1B em vez de 3B)
- Verifique se o dispositivo tem RAM suficiente
- Use navegadores baseados em Chromium para melhor performance

### Problemas de CORS
- Para deploy, configure os headers CORS adequados
- Use um servidor local para desenvolvimento

---

**Desenvolvido com â¤ï¸ usando React e MediaPipe**
